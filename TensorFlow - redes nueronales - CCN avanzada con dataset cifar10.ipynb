{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Una red neuronal convolucional para detectar objetos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import tarfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import urllib\n",
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "output_every = 50\n",
    "generations = 20000\n",
    "eval_every = 500\n",
    "image_height = 32\n",
    "image_width = 32\n",
    "crop_height = 24\n",
    "crop_width = 24\n",
    "num_channels = 3\n",
    "num_targets = 10\n",
    "data_folder = \"cifar-10-batches-bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "caida exponencial del ratio de aprendizaje\n",
    "\n",
    "$$learning\\ rate = 0.1\\cdot 0.9^{\\frac{x}{250}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "lr_decay = 0.9\n",
    "num_gens_to_wait = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_vect_length = image_height*image_width*num_channels\n",
    "record_length = 1+image_vect_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descarga y procesamiento de CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'cifar-10-temp'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "cifar10_url = \"http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\"\n",
    "data_file = os.path.join(data_dir, 'cifar-10-binary.tar.gz')\n",
    "if not os.path.isfile(data_file):\n",
    "    filepath, _ = urllib.request.urlretrieve(cifar10_url, data_file)\n",
    "    tarfile.open(filepath, 'r:gz').extractall(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cifar_files(filename_queue, distort_images = True):\n",
    "    reader = tf.FixedLengthRecordReader(record_bytes=record_length)\n",
    "    key, record_string = reader.read(filename_queue)\n",
    "    #creamos fichero binario\n",
    "    record_bytes = tf.decode_raw(record_string, tf.uint8)\n",
    "    #extraemos la etiqueta\n",
    "    image_label = tf.cast(tf.slice(record_bytes, [0], [1]), tf.int32)\n",
    "    #extraemos la imagen\n",
    "    image_extracted = tf.reshape(tf.slice(record_bytes, [1], [image_vect_length]), [num_channels, image_height, image_width])\n",
    "    #redimensión de imagen\n",
    "    reshaped_image = tf.transpose(image_extracted, [1,2,0])\n",
    "    reshaped_image = tf.cast(reshaped_image, tf.float32)\n",
    "    #crop aleatorio\n",
    "    final_image = tf.image.resize_image_with_crop_or_pad(reshaped_image, crop_width, crop_height)\n",
    "    if distort_images:\n",
    "        #flip aleatorio horizontal, cambiar brillo y contraste\n",
    "        final_image = tf.image.random_flip_left_right(final_image)\n",
    "        final_image = tf.image.random_brightness(final_image, max_delta=63)\n",
    "        final_image = tf.image.random_contrast(final_image, lower=0.2, upper=1.8)\n",
    "    #estandarización por color\n",
    "    final_image = tf.image.per_image_standardization(final_image)\n",
    "    return final_image, image_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_pipeline(batch_size, train_logical=True):\n",
    "    if train_logical:\n",
    "        files = [os.path.join(data_dir, data_folder, 'data_batch_{}.bin'.format(i)) for i in range(1,6)]\n",
    "    else:\n",
    "        files = [os.path.join(data_dir, data_folder, 'test_batch.bin')]\n",
    "    \n",
    "    filename_queue = tf.train.string_input_producer(files)\n",
    "    image, label = read_cifar_files(filename_queue)\n",
    "    \n",
    "    min_after_dequeue = 1000\n",
    "    capacity = min_after_dequeue+3*batch_size\n",
    "    example_batch, label_batch = tf.train.shuffle_batch([image,label], batch_size, capacity, min_after_dequeue)\n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## el modelo CCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_cnn_model(input_images, batch_size, train_logical=True):\n",
    "    def truncated_normal_var(name, shape, dtype):\n",
    "        return tf.get_variable(name=name, shape=shape, dtype=dtype, initializer=tf.truncated_normal_initializer(stddev=0.0))\n",
    "    \n",
    "    def zero_var(name, shape, dtype):\n",
    "        return tf.get_variable(name=name, shape=shape, dtype=dtype, initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    # primera capa de convolucion\n",
    "    with tf.variable_scope(\"conv1\") as scope:\n",
    "        # conv con kernel 5x5 para 3 canales con un total de 64 nodos de salida\n",
    "        conv1_kernel = truncated_normal_var(name=\"conv_kernel1\", shape=[5,5,3,64], dtype=tf.float32)\n",
    "        conv1 = tf.nn.conv2d(input_images, conv1_kernel, [1,1,1,1], padding=\"SAME\")\n",
    "        conv1_bias = zero_var(name=\"conv_bias1\", shape=[64], dtype=tf.float32)\n",
    "        conv1_add_bias = tf.nn.bias_add(conv1, conv1_bias)\n",
    "        # capa de relu\n",
    "        relu_conv1 = tf.nn.relu(conv1_add_bias)\n",
    "    # capa de max pooling  con ventana de 3x3 que se mueve de a 2 lugares cada vez  \n",
    "    pool1 = tf.nn.max_pool(relu_conv1, ksize=[1,3,3,1], strides=[1,2,2,1], padding=\"SAME\", name=\"pool_layer1\")\n",
    "    \n",
    "    # normalizacion local (en ingles: local response normalization(lrn)) sirve para normalizar las imagenes.\n",
    "    norm1 = tf.nn.lrn(pool1, depth_radius=5, bias=2.0, alpha=1e-3, beta=0.75, name=\"norm1\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # segunda capa de convolucion\n",
    "    with tf.variable_scope(\"conv2\") as scope:\n",
    "        # conv con kernel 5x5 para 64 nodos de entrada con un total de 64 nodos de salida\n",
    "        conv2_kernel = truncated_normal_var(name=\"conv_kernel2\", shape=[5,5,64,64], dtype=tf.float32)\n",
    "        conv2 = tf.nn.conv2d(norm1, conv2_kernel, [1,1,1,1], padding=\"SAME\")\n",
    "        conv2_bias = zero_var(name=\"conv_bias2\", shape=[64], dtype=tf.float32)\n",
    "        conv2_add_bias = tf.nn.bias_add(conv2, conv2_bias)\n",
    "        # capa de relu\n",
    "        relu_conv2 = tf.nn.relu(conv2_add_bias)\n",
    "    # capa de max pooling  con ventana de 3x3 que se mueve de a 2 lugares cada vez   \n",
    "    pool2 = tf.nn.max_pool(relu_conv1, ksize=[1,3,3,1], strides=[1,2,2,1], padding=\"SAME\", name=\"pool_layer2\")\n",
    "    \n",
    "    # normalizacion local (en ingles: local response normalization(lrn)) sirve para normalizar las imagenes.\n",
    "    norm2 = tf.nn.lrn(pool2, depth_radius=5, bias=2.0, alpha=1e-3, beta=0.75, name=\"norm2\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # redimensionar a una matriz para poder multiplicar en las capas totalmente conectadas\n",
    "    reshaped_output = tf.reshape(norm2, [batch_size,-1]) #redimensiona la capa anterior para que quede un vector columna y se pueda multiplicar luego\n",
    "    reshaped_dim = reshaped_output.get_shape()[1].value # se obtiene la cantidad de nodos de la imagen, servira para conectar el resultado de \"norm2\" con la capa totalmente conectada\n",
    "    \n",
    "    \n",
    "    \n",
    "    # primera capa totalmente conectada con la cantidad de nodos de entrada que devuleva\"reshaped_dim\" y con 384 nodos de salida\n",
    "    with tf.variable_scope(\"full1\") as scope:\n",
    "        full_weight1 = truncated_normal_var(name=\"full_mult1\", shape=[reshaped_dim,384], dtype=tf.float32)\n",
    "        full_bias1 = zero_var(name=\"full_bias1\", shape=[384], dtype=tf.float32)\n",
    "        full_layer1 = tf.nn.relu(tf.add(tf.matmul(reshaped_output, full_weight1), full_bias1))\n",
    "        \n",
    "        \n",
    "        \n",
    "    # segunda capa totalmente conectada con 384 nodos de entrada y con 192 nodos de salida\n",
    "    with tf.variable_scope(\"full2\") as scope:\n",
    "        full_weight2 = truncated_normal_var(name=\"full_mult2\", shape=[384,192], dtype=tf.float32)\n",
    "        full_bias2 = zero_var(name=\"full_bias2\", shape=[192], dtype=tf.float32)\n",
    "        full_layer2 = tf.nn.relu(tf.add(tf.matmul(full_layer1, full_weight2), full_bias2))    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # tercera capa totalmente conectada con 192 nodos de entrada y con 10 nodos de salida (num_targets=10)\n",
    "    with tf.variable_scope(\"full3\") as scope:\n",
    "        full_weight3 = truncated_normal_var(name=\"full_mult3\", shape=[192,10], dtype=tf.float32)\n",
    "        full_bias3 = zero_var(name=\"full_bias3\", shape=[10], dtype=tf.float32)\n",
    "        final_output = tf.nn.relu(tf.add(tf.matmul(full_layer2, full_weight3), full_bias3))\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_loss(logits, targets):\n",
    "    targets = tf.squeeze(tf.cast(targets, tf.int32)) #elimino las dimensiones inecesarias con squeeze\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=targets)\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    return cross_entropy_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(loss_value, generation_num):\n",
    "    model_learning_rate = tf.train.exponential_decay(learning_rate, generation_num, num_gens_to_wait, lr_decay, staircase=True)\n",
    "    optimizador = tf.train.GradientDescentOptimizer(model_learning_rate)\n",
    "    train_step = optimizador.minimize(loss_value)\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_from_batches(logits, targets):\n",
    "    targets = tf.squeeze(tf.cast(targets, tf.int32)) #elimino las dimensiones inecesarias con squeeze\n",
    "    batch_predictions = tf.cast(tf.argmax(logits,1), tf.int32) # los logits contienen probabilidades de los targets, con \"tf.argmax\" seleccionamos la mas alta\n",
    "    predicted_correctly = tf.equal(batch_predictions, targets) # si son iguales devuelve 1, si son diferentes devuelve 0\n",
    "    accuracy = tf.reduce_mean(tf.cast(predicted_correctly, tf.float32)) # convertimos los V a 1 y los F a 0, y hacemos el promedio\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = input_pipeline(batch_size, train_logical=True) # para entrenamiento\n",
    "test_images, test_targets = input_pipeline(batch_size, train_logical=False) # para test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"model_definition\") as scope:\n",
    "    model_output = cifar_cnn_model(images, batch_size)\n",
    "    scope.reuse_variables()\n",
    "    test_output = cifar_cnn_model(test_images, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = cifar_loss(model_output, targets) # perdida en entrenamiento\n",
    "accuracy = accuracy_from_batches(test_output, test_targets) # accuracy en test\n",
    "generation_num = tf.Variable(0, trainable=False) # indica en que generacion vamos, inicia en 0\n",
    "train_op = train_step(loss, generation_num) # se ejecuta el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "session.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-20-5131d3eb5920>:1: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Thread(QueueRunnerThread-input_producer-input_producer/input_producer_EnqueueMany, started daemon 2912)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 8892)>,\n",
       " <Thread(QueueRunnerThread-input_producer_1-input_producer_1/input_producer_1_EnqueueMany, started daemon 3988)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch_1/random_shuffle_queue-shuffle_batch_1/random_shuffle_queue_enqueue, started daemon 7308)>,\n",
       " <Thread(QueueRunnerThread-input_producer_2-input_producer_2/input_producer_2_EnqueueMany, started daemon 7792)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch_2/random_shuffle_queue-shuffle_batch_2/random_shuffle_queue_enqueue, started daemon 7356)>,\n",
       " <Thread(QueueRunnerThread-input_producer_3-input_producer_3/input_producer_3_EnqueueMany, started daemon 4216)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch_3/random_shuffle_queue-shuffle_batch_3/random_shuffle_queue_enqueue, started daemon 2112)>,\n",
       " <Thread(QueueRunnerThread-input_producer_4-input_producer_4/input_producer_4_EnqueueMany, started daemon 8900)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch_4/random_shuffle_queue-shuffle_batch_4/random_shuffle_queue_enqueue, started daemon 7976)>,\n",
       " <Thread(QueueRunnerThread-input_producer_5-input_producer_5/input_producer_5_EnqueueMany, started daemon 8872)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch_5/random_shuffle_queue-shuffle_batch_5/random_shuffle_queue_enqueue, started daemon 3680)>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.start_queue_runners(sess=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paso 1, perdida: 2.30258\n",
      "paso 2, perdida: 2.30258\n",
      "paso 3, perdida: 2.30258\n",
      "paso 4, perdida: 2.30258\n",
      "paso 5, perdida: 2.30258\n",
      "--- precision en test: 5.47%. ---\n",
      "paso 6, perdida: 2.30258\n",
      "paso 7, perdida: 2.30258\n",
      "paso 8, perdida: 2.30258\n",
      "paso 9, perdida: 2.30258\n",
      "paso 10, perdida: 2.30258\n",
      "--- precision en test: 10.16%. ---\n",
      "paso 11, perdida: 2.30258\n",
      "paso 12, perdida: 2.30258\n",
      "paso 13, perdida: 2.30258\n",
      "paso 14, perdida: 2.30258\n",
      "paso 15, perdida: 2.30258\n",
      "--- precision en test: 3.12%. ---\n",
      "paso 16, perdida: 2.30258\n",
      "paso 17, perdida: 2.30258\n",
      "paso 18, perdida: 2.30258\n",
      "paso 19, perdida: 2.30258\n",
      "paso 20, perdida: 2.30258\n",
      "--- precision en test: 10.94%. ---\n",
      "paso 21, perdida: 2.30258\n",
      "paso 22, perdida: 2.30258\n",
      "paso 23, perdida: 2.30258\n",
      "paso 24, perdida: 2.30258\n",
      "paso 25, perdida: 2.30258\n",
      "--- precision en test: 7.81%. ---\n",
      "paso 26, perdida: 2.30258\n",
      "paso 27, perdida: 2.30258\n",
      "paso 28, perdida: 2.30258\n",
      "paso 29, perdida: 2.30258\n",
      "paso 30, perdida: 2.30258\n",
      "--- precision en test: 9.38%. ---\n",
      "paso 31, perdida: 2.30258\n",
      "paso 32, perdida: 2.30258\n",
      "paso 33, perdida: 2.30258\n",
      "paso 34, perdida: 2.30258\n",
      "paso 35, perdida: 2.30258\n",
      "--- precision en test: 11.72%. ---\n",
      "paso 36, perdida: 2.30258\n",
      "paso 37, perdida: 2.30258\n",
      "paso 38, perdida: 2.30258\n",
      "paso 39, perdida: 2.30258\n",
      "paso 40, perdida: 2.30258\n",
      "--- precision en test: 14.06%. ---\n",
      "paso 41, perdida: 2.30258\n",
      "paso 42, perdida: 2.30258\n",
      "paso 43, perdida: 2.30258\n",
      "paso 44, perdida: 2.30258\n",
      "paso 45, perdida: 2.30258\n",
      "--- precision en test: 11.72%. ---\n",
      "paso 46, perdida: 2.30258\n",
      "paso 47, perdida: 2.30258\n",
      "paso 48, perdida: 2.30258\n",
      "paso 49, perdida: 2.30258\n",
      "paso 50, perdida: 2.30258\n",
      "--- precision en test: 8.59%. ---\n",
      "paso 51, perdida: 2.30258\n",
      "paso 52, perdida: 2.30258\n",
      "paso 53, perdida: 2.30258\n",
      "paso 54, perdida: 2.30258\n",
      "paso 55, perdida: 2.30258\n",
      "--- precision en test: 7.03%. ---\n",
      "paso 56, perdida: 2.30258\n",
      "paso 57, perdida: 2.30258\n",
      "paso 58, perdida: 2.30258\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-3c83abd78d88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "test_acc = []\n",
    "for i in range(20000):\n",
    "    _, loss_value = session.run([train_op, loss])\n",
    "    if(i+1)%50==0:\n",
    "        train_loss.append(loss_value)\n",
    "        print(\"paso {}, perdida: {:.5f}\" .format((i+1), loss_value))\n",
    "\n",
    "    if(i+1)%500==0:\n",
    "        [temp_acc] = session.run([accuracy])\n",
    "        test_acc.append(temp_acc)\n",
    "        print(\"--- precision en test: {:.2f}%. ---\" .format(100*temp_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
